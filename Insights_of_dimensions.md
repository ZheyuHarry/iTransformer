> 这里我们来谈论一下输入的数据在模型中是怎么传播，他的形状又是怎么变化的

# Read the original data
首先在`data_provider.data_loader`中，我们以`Dataset_ETT_hour`类为例，从数据集中读取数据，得到的`df_raw`是一个二维的矩阵，每一行代表一个采样时间，每一列代表一个变量特征。

然后根据`features`我们筛选出需要的列，然后把“date”列提取出来并提取对应的time_feature。

在重新定义的`__getitem__`函数中，我们利用一个时间窗口来生成样本，分别获取时间特征`seq_x_mark` & `seq_y_mark` 和 `seq_x` & `seq_y`

<br>
<br>
<br>
<br>

# Start Training
在上面的这个文件中，我们已经把数据从文件中读取并简单处理了放到了数据加载器中

然后一个batch一个batch的从数据加载器中读取，把数据丢到model之中进行处理

这里还需要注意的是还构造了一个`dec_inp`，用来作为测试和预测阶段的解码输入，它的构造是前面的label_len真实输入，后面的pred_len为全0向量，这是经典的时序预测的构造方法。

> 但是在iTransformer中应该用不上，因为iTransformer是把一个univariate作为token嵌入的


<br>
<br>
<br>
<br>

# Inside the model
OK,现在我们已经把一个batch得出的input和predict数据和对应的时间特征标记都弄出来了，然后丢到model中了。

首先模型会计算统计特征然后归一化，这个统计特征会在最后返还给时序序列，这个是RevIN操作

然后此时输入的`x_enc`是真实时序数据，其形状为`(B , L , N)` ， 分别代表批量(样本数) ， 时间步长度 ， 变量数。

然后我们把它和对应地时间特征`x_mark_enc`一起进行嵌入最后得到形状`(B , N , E)` ， 这个E就是我们希望嵌入的维度，通常就是`d_model` ， 嵌入的维度变化后面单独提

<br>
<br>

然后将这个嵌入的向量传给Encoder进行注意力汇聚，同样得到形状`(B , N , E)` ， 这一部分后面也单独提

现在Encoder已经弄好了，就需要产生输出了，这里直接用一个线性层进行投影，首先通过线性变换得到形状`(B , N , S)` ， 然后交换后两个维度得到`(B , S , N)` ，这个S就是`pred_len`

<strong> 那么到这里为止，我们基本掌握了输入到输出的变换，但是嵌入层和注意力层是怎么变换的请看后面 </strong>

<br>
<br>
<br>
<br>

# Embedding
嵌入层其实很好理解，就是一个可学习的线性层嘛！

由于这个时候是原始数据，形状是`(B , L , N)` , 线性层的形状转变是`c_in -> d_model`，所以先交换数据`x`的后两维，变成`(B , N , L)` ， 如果有时间特征也是同样进行维度转换 ， 然后输入线性层得到形状`(B , N , E)`





<br>
<br>
<br>
<br>

# Attention
这里传入了`(B , N , E)`的向量之后，我们会首先把这个`E`这个维度拆分成多头下的向量，再通过维度变换可以得到`(B , N , H , E')`这个形状，其中`H * E' = E`

这个对于计算queries和keys还有values都是一样的，因为我们传入的时候三者都传了`x`。

然后再计算点积注意力之中就是套用公式了，不过由于我们的Q，K，V都是完全一样的形状，所以最后输出的形状也都是一样的。

这也就是为什么最后的输出向量还是`(B , N , E)`